{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb:\n",
      "  params:\n",
      "    objective: binary\n",
      "    metric: auc\n",
      "    verbose: -1\n",
      "    boosting_type: gbdt\n",
      "    learning_rate: 0.01\n",
      "    max_depth: 5\n",
      "    num_leaves: 31\n",
      "    min_data_in_leaf: 50\n",
      "    bagging_fraction: 0.8\n",
      "    bagging_freq: 1\n",
      "    feature_fraction: 0.8\n",
      "    lambda_l1: 0\n",
      "    lambda_l2: 1\n",
      "    seed: 42\n",
      "  early_stopping_rounds: 100\n",
      "  log_evaluation: 100\n",
      "  num_boost_round: 10000000\n",
      "bert:\n",
      "  params:\n",
      "    model_path: microsoft/deberta-v3-large\n",
      "    metric: auc\n",
      "    target_col_class_num: 2\n",
      "    max_length: 192\n",
      "    fp16: true\n",
      "    learning_rate: 2.0e-05\n",
      "    epochs: 3\n",
      "    per_device_train_batch_size: 8\n",
      "    per_device_eval_batch_size: 32\n",
      "    steps: 2000\n",
      "    lr_scheduler_type: cosine\n",
      "    weight_decay: 0.01\n",
      "exp_number: '012'\n",
      "run_name: base\n",
      "data:\n",
      "  data_root: ../../data\n",
      "  results_root: ../../results\n",
      "  train_path: ../../data/train.csv\n",
      "  clothing_path: ../../data/clothing_master.csv\n",
      "  test_path: ../../data/test.csv\n",
      "  sample_submission_path: ../../data/sample_submission.csv\n",
      "  results_dir: ../../results/012/base\n",
      "seed: 42\n",
      "n_splits: 5\n",
      "target: Recommended IND\n",
      "\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.seed import seed_everything\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "with initialize(config_path=\"config\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    cfg.exp_number = Path().resolve().name\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"25-year-old's review of skirts [sep] title: 3-season skirt! [sep] review text: adorable, well-made skirt! lined and very slimming. i had to size up b/c it runs a bit snug around the waist. however, it's worth it b/c this will match many long and short sleeve tops! [sep] positive feedback count: 4\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 189\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "train_df = pd.read_csv(cfg.data.train_path)\n",
    "clothing_df = pd.read_csv(cfg.data.clothing_path)\n",
    "test_df = pd.read_csv(cfg.data.test_path)\n",
    "\n",
    "if debug:\n",
    "    train_df = train_df.sample(1000, random_state=cfg.seed).reset_index(drop=True)\n",
    "\n",
    "train_df = pd.merge(train_df, clothing_df, on=\"Clothing ID\", how=\"left\")\n",
    "test_df = pd.merge(test_df, clothing_df, on=\"Clothing ID\", how=\"left\")\n",
    "all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "# 特徴量作成\n",
    "grp_col = \"Clothing ID\"\n",
    "num_features = [\"Age\", \"Positive Feedback Count\"]\n",
    "grp_df = all_df.groupby(grp_col)[num_features].mean().map(int).reset_index()\n",
    "grp_df.columns = [grp_col] + [f\"{col}_grp_mean\" for col in num_features]\n",
    "train_df = pd.merge(train_df, grp_df, on=grp_col, how=\"left\")\n",
    "test_df = pd.merge(test_df, grp_df, on=grp_col, how=\"left\")\n",
    "\n",
    "# prompt作成\n",
    "def preprocess(df):\n",
    "    df['prompt'] = (\n",
    "        df['Age'].map(str) + \"-Year-Old's Review of \" + df['Class Name'] + ' [SEP] TITLE: ' + df['Title'].fillna('none') + ' [SEP] Review Text: ' + df['Review Text'].fillna('none') + ' [SEP] Positive Feedback Count: ' + df['Positive Feedback Count'].map(str)\n",
    "    )\n",
    "    df[\"prompt\"] = df[\"prompt\"].str.lower()\n",
    "    return df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "train_df[\"labels\"] = train_df[cfg.target].astype(np.int8)\n",
    "\n",
    "display(train_df[\"prompt\"][0])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.bert.params.model_path)\n",
    "train_max_length = train_df[\"prompt\"].map(lambda x: len(tokenizer(x)[\"input_ids\"])).max()\n",
    "test_max_length = test_df[\"prompt\"].map(lambda x: len(tokenizer(x)[\"input_ids\"])).max()\n",
    "\n",
    "print(train_max_length, test_max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoth Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"none\", alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1.0 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\", alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction=\"none\", alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_fn=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_fn is None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            # ロジットとラベルの形状を調整\n",
    "            logits = logits[:, 1]  # ポジティブクラスのロジットのみを使用\n",
    "            labels = labels.float()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Smooth Focal Lossのインスタンスを作成\n",
    "loss_fn = SmoothFocalLoss(alpha=1, gamma=2, smoothing=0)  # 結局gammaしか効かしてない\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adfe27b20294017a5ee3ef3b0d47171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b2d6128f67480996c8b6c7643ac323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 19:53, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=936, training_loss=0.048035743909004405, metrics={'train_runtime': 1196.0632, 'train_samples_per_second': 25.082, 'train_steps_per_second': 0.783, 'total_flos': 8328260851977216.0, 'train_loss': 0.048035743909004405, 'epoch': 2.9952})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = torch.softmax(torch.tensor(preds), dim = 1).numpy()\n",
    "    score = roc_auc_score(labels, preds[:, 1])\n",
    "    return {'auc': score}\n",
    "\n",
    "# 実験結果格納用のディレクトリを作成\n",
    "cfg.run_name = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "Path(cfg.data.results_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "y_train = train_df[cfg.target]\n",
    "oof = np.zeros(len(train_df))\n",
    "\n",
    "ds_train = Dataset.from_pandas(train_df[['prompt', 'labels']].copy())\n",
    "ds_eval = Dataset.from_pandas(train_df[['prompt', 'labels']].copy())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.bert.params.model_path)\n",
    "config = AutoConfig.from_pretrained(cfg.bert.params.model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.bert.params.model_path, config=config)\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['prompt'], max_length=cfg.bert.params.max_length, truncation=True)\n",
    "\n",
    "ds_train = ds_train.map(tokenize).remove_columns(['prompt'])\n",
    "ds_eval = ds_eval.map(tokenize).remove_columns(['prompt'])\n",
    "\n",
    "output_dir = f\"{cfg.data.results_dir}/all\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    fp16=cfg.bert.params.fp16,\n",
    "    learning_rate=cfg.bert.params.learning_rate,\n",
    "    num_train_epochs=cfg.bert.params.epochs,\n",
    "    per_device_train_batch_size=cfg.bert.params.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.bert.params.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=cfg.bert.params.steps,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=cfg.bert.params.steps,\n",
    "    logging_steps=cfg.bert.params.steps,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=cfg.bert.params.lr_scheduler_type,\n",
    "    metric_for_best_model=cfg.bert.params.metric,\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=cfg.bert.params.weight_decay,\n",
    "    save_safetensors=True,\n",
    "    seed=cfg.seed,\n",
    "    data_seed=cfg.seed,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_fn=loss_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../results/012/20240831_051430/all/final/tokenizer_config.json',\n",
       " '../../results/012/20240831_051430/all/final/special_tokens_map.json',\n",
       " '../../results/012/20240831_051430/all/final/spm.model',\n",
       " '../../results/012/20240831_051430/all/final/added_tokens.json',\n",
       " '../../results/012/20240831_051430/all/final/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_dir = f\"{output_dir}/final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# oof_df = pd.DataFrame({\"oof\": oof})\n",
    "# oof_df.to_csv(f\"{cfg.data.results_dir}/oof.csv\", index=False)\n",
    "\n",
    "# best_score = roc_auc_score(y_train, oof)\n",
    "# with open(f\"{cfg.data.results_dir}/log.txt\", \"w\") as log_file:\n",
    "#     log_file.write(\"====== CV Score ======\\n\")\n",
    "#     log_file.write(f\"best_score: {best_score}\\n\")\n",
    "#     log_file.write(\"\\n====== params ======\\n\")\n",
    "#     log_file.write(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 5))\n",
    "# sns.histplot(y_train, bins=50)\n",
    "# sns.histplot(oof, bins=50)\n",
    "# plt.legend([\"true\", \"oof\"])\n",
    "# plt.show()\n",
    "# fig.savefig(f\"{cfg.data.results_dir}/oof_hist.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUbmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fcdb96b5994df7b48b0c82028df4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.419707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.967148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.962391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0  0.964472\n",
       "1  0.419707\n",
       "2  0.967148\n",
       "3  0.396068\n",
       "4  0.962391"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['prompt'], max_length=cfg.bert.params.max_length, truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_output_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(final_output_dir)\n",
    "\n",
    "ds_test = Dataset.from_pandas(test_df[['prompt']].copy())\n",
    "ds_test = ds_test.map(tokenize).remove_columns(['prompt'])\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir=cfg.data.results_dir,\n",
    "    per_device_eval_batch_size=cfg.bert.params.per_device_eval_batch_size,\n",
    "    do_predict=True,\n",
    "    dataloader_drop_last=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "predictions = torch.softmax(torch.tensor(trainer.predict(ds_test).predictions), dim=1).numpy()\n",
    "\n",
    "pred = predictions[:, 1]\n",
    "\n",
    "# 提出用ファイル作成\n",
    "sub_df = pd.read_csv(cfg.data.sample_submission_path)\n",
    "sub_df[\"target\"] = pred\n",
    "sub_df.to_csv(f\"{cfg.data.results_dir}/{cfg.run_name}_submission.csv\", index=False)\n",
    "sub_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
